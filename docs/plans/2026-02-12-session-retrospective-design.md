# Session Retrospective Skill â€” Design

## Overview

A single-file skill invoked at end-of-session that analyzes process efficiency across 5 categories, scores each 1â€“5, and writes a structured markdown report to disk.

## Design Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Architecture | Single-pass inline | Model already has session context; no need to extract/pass it |
| Data sources | Context + CLI metrics + log files | Maximize insight from all available data |
| Output | Saved markdown report | Historical record for trend tracking |
| Timing | End-of-session only | Full session analysis, simpler scope |
| Scoring | 1â€“5 numerical + qualitative | Enables trend tracking across sessions |
| Scope | Process-focused | Context engineering, tools, agents, cost â€” not outcome quality |

## Skill Structure

```
.claude/skills/session-retrospective/
  SKILL.md          # Main skill prompt (single file)
```

**Frontmatter:**
```yaml
---
name: session-retrospective
description: End-of-session process analysis. Scores context engineering, tool usage, sub-agents, cost, and provides improvement insights. Writes report to ~/notes/retrospectives/.
---
```

## Invocation Flow

```
Invoke /session-retrospective
  â†’ Announce start
  â†’ Collect CLI/log metrics via Bash
  â†’ Scan available skills list (detect missed opportunities)
  â†’ Analyze session context across 5 categories using embedded heuristics
  â†’ Score each category 1â€“5 with qualitative narrative
  â†’ Write report to disk
  â†’ Print inline summary: scorecard table + top 3 takeaways
```

## Report Location

```
~/notes/retrospectives/<repo-name>/YYYY-MM-DD-HHMMSS.md
```

Timestamped to the second to avoid collisions from multiple sessions per day.

## Report Format

```markdown
# Session Retrospective â€” <repo> â€” <date>

## Session Summary
- **Goal**: [What the session set out to accomplish]
- **Duration**: [Approximate based on timestamps if available]
- **Outcome**: [Brief factual summary of what was accomplished]

## Scorecard

| Category | Score | Grade |
|----------|-------|-------|
| Context Engineering | X/5 | ðŸŸ¢/ðŸŸ¡/ðŸ”´ |
| Tool Usage | X/5 | ðŸŸ¢/ðŸŸ¡/ðŸ”´ |
| Sub-agent Work | X/5 | ðŸŸ¢/ðŸŸ¡/ðŸ”´ |
| Cost Efficiency | X/5 | ðŸŸ¢/ðŸŸ¡/ðŸ”´ |
| **Overall** | **X.X/5** | **ðŸŸ¢/ðŸŸ¡/ðŸ”´** |

## 1. Context Engineering (X/5)
### What went well
### What could improve
### Key metrics

## 2. Tool Usage (X/5)
### What went well
### What could improve
### Tool inventory table

## 3. Sub-agent Work (X/5)
### What went well
### What could improve
### Agent inventory table

## 4. Cost Efficiency (X/5)
### Metrics
### What could improve

## 5. Actionable Insights
### Prompt improvements
### Skill improvements
### Process improvements

## Top 3 Takeaways
```

## Scoring Rubric

| Score | Meaning |
|-------|---------|
| 5 | Excellent â€” near-optimal choices, minimal waste |
| 4 | Good â€” mostly effective with minor improvements possible |
| 3 | Adequate â€” functional but notable inefficiencies |
| 2 | Below average â€” significant missed opportunities |
| 1 | Poor â€” major process failures or waste |

**Grade mapping**: 4â€“5 = ðŸŸ¢, 3 = ðŸŸ¡, 1â€“2 = ðŸ”´

## Data Collection

### From conversation context (no tool calls)
- Tool calls and results (Read, Grep, Glob, Edit, Write, Bash, Task, Skill, etc.)
- Skill invocations and which skills were loaded
- Subagent launches (Task tool with subagent_type, run_in_background, etc.)
- AskUserQuestion interactions
- Error messages, retries, and course corrections
- Initial user prompt and mid-session steering

### From Bash commands
- Cost data from Claude CLI (if available)
- Session log files from ~/.claude/
- Git activity during session
- Available skills list (for missed opportunity detection)

## Analysis Heuristics

### Context Engineering
- Multiple Read calls on the same file â†’ duplication
- Large file reads without offset/limit â†’ inefficiency
- Grep/Glob when Explore agent would have been better â†’ wrong tool
- Skills loaded that weren't relevant â†’ wasted context

### Tool Usage
- Bash for file reading instead of Read tool â†’ anti-pattern
- Direct Grep/Glob instead of Explore agent for open-ended searches â†’ inefficiency
- Available skills not invoked when applicable â†’ missed opportunity

### Sub-agent Work
- Sequential tool calls that could have been parallelized â†’ missed parallelization
- Long exploration in main context vs. delegating to Explore â†’ context waste
- TaskOutput called on background agents â†’ anti-pattern (per CLAUDE.md)

### Cost Efficiency
- Haiku-eligible tasks run on Opus â†’ cost flag
- Large file reads where only a portion was needed â†’ waste
- Duplicate information gathering â†’ waste
